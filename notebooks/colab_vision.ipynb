{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# JurisCape Node A: The Vision Agent (Qwen2.5-VL + PaddleOCR)\n",
                "**Role**: Analyze Images/Videos. Use OCR tool if timestamps are detected.\n",
                "**Model**: `Qwen/Qwen2.5-VL-7B-Instruct` (Quantized).\n",
                "**Tool**: `PaddleOCR` (Multi-language Support)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install fastapi uvicorn pyngrok python-multipart nest_asyncio requests\n",
                "!pip install git+https://github.com/huggingface/transformers accelerate bitsandbytes\n",
                "!pip install paddlepaddle-gpu paddleocr opencv-python-headless\n",
                "!apt-get install ffmpeg libsm6 libxext6  -y"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load PaddleOCR\n",
                "from paddleocr import PaddleOCR\n",
                "print(\"Loading PaddleOCR...\")\n",
                "# Lang='en' supports english. Paddle is auto-detect mostly. \n",
                "# For specific indian languages, we might need specific lang codes if auto fails.\n",
                "ocr_engine = PaddleOCR(use_angle_cls=True, lang='en') \n",
                "print(\"PaddleOCR Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load Qwen2.5-VL (Vision Language Model)\n",
                "import torch\n",
                "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
                "\n",
                "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
                "print(f\"Loading {model_id}...\")\n",
                "\n",
                "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
                "    model_id,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    attn_implementation=\"flash_attention_2\",\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "processor = AutoProcessor.from_pretrained(model_id)\n",
                "print(\"Qwen2.5-VL Loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define Helper Functions (Video/Image Processing)\n",
                "import cv2\n",
                "import numpy as np\n",
                "import re\n",
                "\n",
                "def extract_frame_at_timestamp(video_path, timestamp_str):\n",
                "    \"\"\"Extracts a frame at HH:MM:SS or MM:SS\"\"\"\n",
                "    # Convert timestamp to seconds\n",
                "    parts = list(map(int, timestamp_str.split(':')))\n",
                "    seconds = 0\n",
                "    if len(parts) == 3:\n",
                "        seconds = parts[0]*3600 + parts[1]*60 + parts[2]\n",
                "    elif len(parts) == 2:\n",
                "        seconds = parts[0]*60 + parts[1]\n",
                "    \n",
                "    cap = cv2.VideoCapture(video_path)\n",
                "    cap.set(cv2.CAP_PROP_POS_MSEC, (seconds * 1000))\n",
                "    ret, frame = cap.read()\n",
                "    cap.release()\n",
                "    if ret:\n",
                "        return frame\n",
                "    return None\n",
                "\n",
                "def run_ocr_on_frame(frame):\n",
                "    result = ocr_engine.ocr(frame, cls=True)\n",
                "    # Flatten result\n",
                "    text = \"\\n\".join([line[1][0] for line in result[0]])\n",
                "    return text\n",
                "\n",
                "def run_qwen(content_inputs, prompt_text):\n",
                "    # Prepare inputs for Qwen\n",
                "    messages = [\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": content_inputs + [{\"type\": \"text\", \"text\": prompt_text}]\n",
                "        }\n",
                "    ]\n",
                "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = processor(text=[text], images=..., videos=..., padding=True, return_tensors=\"pt\") \n",
                "    # Note: Actual image/video binding logic needs detailed implementation matching HuggingFace docs for Qwen2-VL\n",
                "    # Simplified for snippet length. Assuming global processor handles the inputs correctly \n",
                "    # or we pre-process images/videos into the format Qwen expects.\n",
                "    \n",
                "    inputs = inputs.to(\"cuda\")\n",
                "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
                "    return processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
                "\n",
                "# Note: The above run_qwen is pseudo-code-ish for the binding part due to complexity. \n",
                "# Real implementation requires 'vision_infos' handling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. The AGENTIC LOOP\n",
                "async def process_media_agentic(file_path):\n",
                "    # PASS 1: Initial Scan\n",
                "    print(\"Running Pass 1 (Qwen Visual Scan)...\")\n",
                "    # In real code: Load video/image into Qwen input format\n",
                "    initial_response = \"[MOCK] I see a car with license plate number visible at <<<12:05>>>.\"\n",
                "    \n",
                "    # Check for Tool Trigger\n",
                "    timestamp_match = re.search(r'<<<(\\d{1,2}:\\d{2}(?::\\d{2})?)>>>', initial_response)\n",
                "    \n",
                "    if timestamp_match:\n",
                "        timestamp = timestamp_match.group(1)\n",
                "        print(f\"Tool Triggered: OCR at {timestamp}\")\n",
                "        \n",
                "        frame = extract_frame_at_timestamp(file_path, timestamp)\n",
                "        if frame is not None:\n",
                "            ocr_text = run_ocr_on_frame(frame)\n",
                "            print(f\"OCR Result: {ocr_text}\")\n",
                "            \n",
                "            # PASS 2: Re-run with Context\n",
                "            print(\"Running Pass 2 (Qwen + Context)...\")\n",
                "            final_response = f\"{initial_response}\\n[OCR DATA at {timestamp}]: {ocr_text}\"\n",
                "            return final_response\n",
                "            \n",
                "    return initial_response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Start Server\n",
                "from pyngrok import ngrok\n",
                "import uvicorn\n",
                "import os\n",
                "from fastapi import FastAPI, Request\n",
                "from pydantic import BaseModel\n",
                "\n",
                "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
                "ngrok.set_auth_token(NGROK_TOKEN)\n",
                "SWARM_SECRET = \"change-me-in-prod-secure-swarm-key\"\n",
                "\n",
                "app = FastAPI()\n",
                "\n",
                "class VisualRequest(BaseModel):\n",
                "    file_url: str\n",
                "\n",
                "@app.post(\"/analyze_vision\")\n",
                "async def analyze_vision(payload: VisualRequest):\n",
                "    # Mock download logic for demo\n",
                "    local_path = \"test_video.mp4\"\n",
                "    return {\"description\": await process_media_agentic(local_path)}\n",
                "\n",
                "# CLEANUP & RUN\n",
                "ngrok.kill()\n",
                "os.system(\"pkill ngrok\")\n",
                "tunnel = ngrok.connect(8000)\n",
                "print(f\"\\n=== PUBLIC URL: {tunnel.public_url} ===\\n\")\n",
                "config = uvicorn.Config(app, port=8000)\n",
                "await uvicorn.Server(config).serve()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}