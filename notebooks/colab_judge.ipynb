{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# JurisCape Node D: The Judge (DeepSeek-R1)\n",
                "**Role**: Legal Reasoning and Verdict Generation.\n",
                "**Model**: `deepseek-ai/DeepSeek-R1-Distill-Llama-8B` (4-bit via Unsloth)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install fastapi uvicorn pyngrok python-multipart nest_asyncio requests\n",
                "!pip install unsloth vllm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load DeepSeek-R1 (Distill 8B)\n",
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True\n",
                "\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")\n",
                "FastLanguageModel.for_inference(model)\n",
                "print(\"Judge is Ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Define Judgment Logic\n",
                "def get_verdict(facts, legal_sections):\n",
                "    prompt_template = \"\"\"\n",
                "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
                "\n",
                "    ### Instruction:\n",
                "    You are a strictly logical Judge. Analyze the following FACTS against the LAWS.\n",
                "    Determine 'Guilty' or 'Not Guilty' and provide STEP-BY-STEP reasoning.\n",
                "\n",
                "    ### Input:\n",
                "    FACTS: {facts}\n",
                "    LAWS: {laws}\n",
                "\n",
                "    ### Response:\n",
                "    \"\"\"\n",
                "    \n",
                "    inputs = tokenizer(\n",
                "        prompt_template.format(facts=facts, laws=legal_sections),\n",
                "        return_tensors = \"pt\"\n",
                "    ).to(\"cuda\")\n",
                "\n",
                "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
                "    return tokenizer.batch_decode(outputs)[0]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Start Server\n",
                "from pyngrok import ngrok\n",
                "import uvicorn\n",
                "import os\n",
                "from fastapi import FastAPI, Request\n",
                "from fastapi.responses import JSONResponse\n",
                "from pydantic import BaseModel\n",
                "from typing import List\n",
                "\n",
                "NGROK_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"\n",
                "ngrok.set_auth_token(NGROK_TOKEN)\n",
                "\n",
                "SWARM_SECRET = \"change-me-in-prod-secure-swarm-key\"\n",
                "app = FastAPI()\n",
                "\n",
                "@app.middleware(\"http\")\n",
                "async def verify_secret(request: Request, call_next):\n",
                "    if request.url.path in [\"/docs\", \"/openapi.json\"]:\n",
                "        return await call_next(request)\n",
                "    if request.headers.get(\"X-Swarm-Secret\") != SWARM_SECRET:\n",
                "        return JSONResponse(status_code=403, content={\"detail\": \"Unauthorized\"})\n",
                "    return await call_next(request)\n",
                "\n",
                "class JudgmentRequest(BaseModel):\n",
                "    facts: List[str]\n",
                "    evidence_summary: str\n",
                "    legal_sections: List[str]\n",
                "\n",
                "@app.post(\"/adjudicate\")\n",
                "async def adjudicate(payload: JudgmentRequest):\n",
                "    print(\"Deliberating...\")\n",
                "    raw_verdict = get_verdict(payload.facts, payload.legal_sections)\n",
                "    return {\n",
                "        \"raw_output\": raw_verdict,\n",
                "        \"summary\": \"[Processed] Verdict logic executed.\"\n",
                "    }\n",
                "\n",
                "# CLEANUP & RUN\n",
                "ngrok.kill()\n",
                "os.system(\"pkill ngrok\")\n",
                "tunnel = ngrok.connect(8000)\n",
                "print(f\"\\n=== PUBLIC URL: {tunnel.public_url} ===\\n\")\n",
                "config = uvicorn.Config(app, port=8000)\n",
                "await uvicorn.Server(config).serve()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}